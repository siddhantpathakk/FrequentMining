{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Initialisation and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import itertools\n",
    "import time\n",
    "from colorama import Fore\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mining based utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_count_dict(items, support_value):\n",
    "    item_counts = {}\n",
    "    for item in items:\n",
    "        if item not in item_counts.keys():\n",
    "            item_counts[item] = 1\n",
    "        else:\n",
    "            if item_counts[item] < support_value:\n",
    "                item_counts[item] = item_counts[item] + 1\n",
    "    return item_counts\n",
    "\n",
    "\n",
    "def create_count_dict_tuple(chunk, support_value, candidate_tuples):\n",
    "    tuple_counts = {}\n",
    "    for basket in chunk:\n",
    "        for candidate in candidate_tuples:\n",
    "            if set(candidate).issubset(basket):\n",
    "                if candidate in tuple_counts and tuple_counts[candidate] < support_value:\n",
    "                    tuple_counts[candidate] += 1\n",
    "                elif candidate not in tuple_counts:\n",
    "                    tuple_counts[candidate] = 1\n",
    "    return tuple_counts\n",
    "\n",
    "\n",
    "def filter_by_support(count_dict, support_value, is_tuple):\n",
    "    frequent_candidates = []\n",
    "    for candidate, count in count_dict.items():\n",
    "        if count >= support_value:\n",
    "            frequent_candidates.append(candidate)\n",
    "            phase1_candidates.append((candidate if is_tuple else tuple({candidate}), 1))\n",
    "    return frequent_candidates\n",
    "\n",
    "\n",
    "def get_frequent_candidate_tuples(frequent_items, size):\n",
    "    candidates = []\n",
    "    for item_x in frequent_items:\n",
    "        for item_y in frequent_items:\n",
    "            combined_set = tuple(sorted(set(item_x + item_y)))\n",
    "            if len(combined_set) == size:\n",
    "                if combined_set not in candidates:\n",
    "                    previous_candidates = list(itertools.combinations(combined_set, size - 1))\n",
    "                    flag = True\n",
    "                    for candidate in previous_candidates:\n",
    "                        if candidate not in frequent_items:\n",
    "                            flag = False\n",
    "                            break\n",
    "                    if flag:\n",
    "                        candidates.append(combined_set)\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def find_frequent_candidates(chunk, chunk_support, frequent_items, size):\n",
    "    if size == 2:\n",
    "        candidates = list(itertools.combinations(sorted(frequent_items), 2))\n",
    "    else:\n",
    "        candidates = get_frequent_candidate_tuples(frequent_items, size)\n",
    "    return filter_by_support(create_count_dict_tuple(chunk, chunk_support, candidates), chunk_support, True)\n",
    "\n",
    "\n",
    "def apriori(baskets):\n",
    "    chunk = list(baskets)\n",
    "    chunk_support = support * (len(chunk) / basket_count)\n",
    "    items = []\n",
    "    for basket in chunk:\n",
    "        for item in basket:\n",
    "            items.append(item)\n",
    "\n",
    "    #frequent itemsets of size 1\n",
    "    frequent_items = filter_by_support(create_count_dict(items, chunk_support), chunk_support, False)\n",
    "    size = 2\n",
    "    while len(frequent_items) != 0:\n",
    "        frequent_items = find_frequent_candidates(chunk, chunk_support, frequent_items, size)\n",
    "        size += 1\n",
    "\n",
    "    return [phase1_candidates]\n",
    "\n",
    "def son(candidate):\n",
    "    frequent_itemset_count = {}\n",
    "    for basket in basket_list:\n",
    "        if isinstance(candidate, tuple):\n",
    "            if set(candidate).issubset(basket):\n",
    "                if candidate in frequent_itemset_count and frequent_itemset_count[candidate] < support:\n",
    "                    frequent_itemset_count[candidate] += 1\n",
    "                elif candidate not in frequent_itemset_count:\n",
    "                    frequent_itemset_count[candidate] = 1\n",
    "        else:\n",
    "            frequent_itemset_count = create_count_dict(basket, support)\n",
    "\n",
    "    frequent_itemsets_actual = []\n",
    "    for itemset, count in frequent_itemset_count.items():\n",
    "        frequent_itemsets_actual.append((itemset, count))\n",
    "    return frequent_itemsets_actual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I/O based utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_required_bucket(row):\n",
    "    line = row.split(\",\")\n",
    "    return line[0], line[1]\n",
    "\n",
    "\n",
    "def write_to_file(candidates, frequent_itemsets, output_file):\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write(\"Candidates:\\n\" + candidates + \"\\n\\nFrequent Itemsets:\\n\" + frequent_itemsets)\n",
    "\n",
    "\n",
    "def get_output_string(input_tuple, size, output):\n",
    "    if len(input_tuple) > size:\n",
    "        output = output[:-1] + \"\\n\\n\"\n",
    "\n",
    "    output = output + \"('\" + str(input_tuple[0]) + \"'),\" if len(input_tuple) == 1 else output + str(input_tuple) + \",\"\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_output(phase_rdd, is_phase_2):\n",
    "    size = 1\n",
    "    output = \"\"\n",
    "    sorted_list = sorted(sorted(phase_rdd if is_phase_2 else phase_rdd.map(lambda x: x[0]).collect()), key=len)\n",
    "    for x in sorted_list:\n",
    "        output = get_output_string(x, size, output)\n",
    "        size = len(x)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Datasets chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_metadata = {\n",
    "    'amazon-reviews': {\n",
    "        'path': './data/amazon-reviews/transactions3.csv',\n",
    "        'output_dir': './logs/amazon-reviews/',\n",
    "        'minsup': [23305530],\n",
    "        'column_names': 'reviewerId,itemId'\n",
    "    },\n",
    "    \n",
    "    \n",
    "    'groceries': {\n",
    "        'path': './data/groceries/transactions.csv',\n",
    "        'output_dir': './logs/groceries/',\n",
    "        'column_names': 'userId,itemName',\n",
    "        'minsup': [250, 200, 150, 100, 50],\n",
    "    },\n",
    "    \n",
    "    \n",
    "    'movielens': {\n",
    "        'path': './data/movielens/transactions.csv',\n",
    "        'output_dir': './logs/movielens/',\n",
    "        'column_names': 'userId,movieId',\n",
    "        'minsup': [250, 200, 150, 100, 50],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'amazon-reviews'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23305530]\n"
     ]
    }
   ],
   "source": [
    "input_filepath = dataset_metadata[dataset_name]['path']\n",
    "output_dir = dataset_metadata[dataset_name]['output_dir']\n",
    "minsup_values_list = dataset_metadata[dataset_name]['minsup']\n",
    "column_names = dataset_metadata[dataset_name]['column_names']\n",
    "\n",
    "print(minsup_values_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mRunning SON Apriori Algorithm on \u001b[32mamazon-reviews \u001b[37mwith minsup value: 23305530\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/20 06:41:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/11/20 06:41:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read file into rdd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 06:43:49 ERROR Executor: Exception in task 28.0 in stage 0.0 (TID 28)8]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:776)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\n",
      "\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1214)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1383/1469039650.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/11/20 06:43:49 ERROR Executor: Exception in task 31.0 in stage 0.0 (TID 31)\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:776)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\n",
      "\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1214)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1383/1469039650.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/11/20 06:43:52 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 31.0 in stage 0.0 (TID 31),5,main]\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:776)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\n",
      "\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1214)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1383/1469039650.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/11/20 06:43:52 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 28.0 in stage 0.0 (TID 28),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:776)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\n",
      "\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1214)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1383/1469039650.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/11/20 06:43:52 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@2dedb008 rejected from java.util.concurrent.ThreadPoolExecutor@87da4b4[Shutting down, pool size = 72, active threads = 72, queued tasks = 0, completed tasks = 0]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:363)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/11/20 06:43:52 ERROR TaskSetManager: Task 31 in stage 0.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSpark Server stopped due to exception.\n",
      "\u001b[31mError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 31 in stage 0.0 failed 1 times, most recent failure: Lost task 31.0 in stage 0.0 (TID 31) (SCSEGPU-TC1.cm.cluster executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:776)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\n",
      "\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1214)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1383/1469039650.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:776)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\n",
      "\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1214)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1383/1469039650.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 06:43:53 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$2080/1711697646@6c69ef83 rejected from java.util.concurrent.ThreadPoolExecutor@62ae1c7e[Shutting down, pool size = 4, active threads = 0, queued tasks = 0, completed tasks = 20]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:838)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/11/20 06:43:53 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$2080/1711697646@63a297c8 rejected from java.util.concurrent.ThreadPoolExecutor@62ae1c7e[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 20]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:838)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/11/20 06:43:53 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$2080/1711697646@ef902b9 rejected from java.util.concurrent.ThreadPoolExecutor@62ae1c7e[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 20]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:838)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/11/20 06:43:53 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$2080/1711697646@544875bf rejected from java.util.concurrent.ThreadPoolExecutor@62ae1c7e[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 20]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:838)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/FYP/siddhant005/.conda/envs/dam_proj2/lib/python3.12/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/FYP/siddhant005/.conda/envs/dam_proj2/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/FYP/siddhant005/.conda/envs/dam_proj2/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "phase1_candidates = []\n",
    "support = minsup_values_list[0]\n",
    "\n",
    "print(Fore.WHITE + f'Running SON Apriori Algorithm on', Fore.GREEN + f'{dataset_name}', \n",
    "        Fore.WHITE + f'with minsup value: {support}\\n')\n",
    "\n",
    "SparkContext.setSystemProperty(\"spark.executor.memory\", \"20g\")\n",
    "SparkContext.setSystemProperty(\"spark.driver.memory\", \"20g\")\n",
    "spark_configuration = SparkConf().setAppName(\"cz4032_task2_v2\").setMaster('local[*]')\n",
    "sc = SparkContext(conf=spark_configuration)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "try:\n",
    "    rdd = sc.textFile(input_filepath)\n",
    "    print('read file into rdd')\n",
    "\n",
    "    basket_rdd = rdd.filter(\n",
    "        lambda x: x != column_names).map(\n",
    "            lambda x: get_required_bucket(x)\n",
    "            ).groupByKey().mapValues(set).map(\n",
    "                lambda x: x[1]\n",
    "                ).persist()\n",
    "        \n",
    "    \n",
    "    basket_list = basket_rdd.collect()\n",
    "    \n",
    "    basket_count = len(basket_list)\n",
    "    \n",
    "    phase1_map = basket_rdd.mapPartitions(apriori).flatMap(lambda x: x)\n",
    "    \n",
    "    phase1_reduce = phase1_map.reduceByKey(lambda x, y: x + y).persist()\n",
    "    \n",
    "    phase2_map = phase1_reduce.map(lambda x: son(x[0])).flatMap(lambda x: x)\n",
    "    \n",
    "    phase2_reduce = phase2_map.filter(lambda x: x[1] >= support).map(lambda x: x[0]).collect()\n",
    "    \n",
    "    phase1_output = get_output(phase1_reduce, False)[:-1]\n",
    "    phase2_output = get_output(phase2_reduce, True)[:-1]\n",
    "\n",
    "    sc.stop()\n",
    "    \n",
    "    basket_rdd.unpersist()\n",
    "    phase1_map.unpersist()\n",
    "    phase1_reduce.unpersist()\n",
    "    phase2_map.unpersist()\n",
    "    \n",
    "    output_filepath = f'{output_dir}{dataset_name}_minsup_{support}.txt'\n",
    "    write_to_file(phase1_output, phase2_output, output_filepath)\n",
    "    \n",
    "    print(Fore.BLUE + f'--'*30)\n",
    "    print(Fore.WHITE + f\"| MINSUP:\",Fore.GREEN + f\"{support}\")\n",
    "    print(Fore.WHITE + f\"| Time taken:\", Fore.GREEN + f\"{round(time.time() - start, 1)} seconds.\")\n",
    "    print(Fore.WHITE + f\"| Outputs saved to\", Fore.GREEN +  f\"{output_filepath}.\")\n",
    "    print(Fore.BLUE + f'--'*30)\n",
    "    \n",
    "except Exception as e:\n",
    "    sc.stop()\n",
    "    print(Fore.RED + f'Spark Server stopped due to exception.')\n",
    "    print(Fore.RED + f'Error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mRunning SON Apriori Algorithm on \u001b[32mgroceries \u001b[37mwith minsup value: 200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m------------------------------------------------------------\n",
      "\u001b[37m| MINSUP: \u001b[32m200\n",
      "\u001b[37m| Time taken: \u001b[32m4.2 seconds.\n",
      "\u001b[37m| Outputs saved to \u001b[32m./logs/groceries/groceries_minsup_200.txt.\n",
      "\u001b[34m------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "phase1_candidates = []\n",
    "support = minsup_values_list[1]\n",
    "\n",
    "print(Fore.WHITE + f'Running SON Apriori Algorithm on', Fore.GREEN + f'{dataset_name}', \n",
    "        Fore.WHITE + f'with minsup value: {support}\\n')\n",
    "\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '15g')\n",
    "spark_configuration = SparkConf().setAppName(\"cz4032_task2\").setMaster('local[*]')\n",
    "sc = SparkContext(conf=spark_configuration)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "try:\n",
    "    rdd = sc.textFile(input_filepath)\n",
    "    \n",
    "\n",
    "    basket_rdd = rdd.filter(\n",
    "        lambda x: x != column_names).map(\n",
    "            lambda x: get_required_bucket(x)\n",
    "            ).groupByKey().mapValues(set).map(\n",
    "                lambda x: x[1]\n",
    "                ).persist()\n",
    "        \n",
    "    basket_list = basket_rdd.collect()\n",
    "    basket_count = len(basket_list)\n",
    "    \n",
    "    phase1_map = basket_rdd.mapPartitions(apriori).flatMap(lambda x: x)\n",
    "    phase1_reduce = phase1_map.reduceByKey(lambda x, y: x + y).persist()\n",
    "\n",
    "    phase2_map = phase1_reduce.map(lambda x: son(x[0])).flatMap(lambda x: x)\n",
    "    phase2_reduce = phase2_map.filter(lambda x: x[1] >= support).map(lambda x: x[0]).collect()\n",
    "\n",
    "    phase1_output = get_output(phase1_reduce, False)[:-1]\n",
    "    phase2_output = get_output(phase2_reduce, True)[:-1]\n",
    "\n",
    "    sc.stop()\n",
    "    \n",
    "    output_filepath = f'{output_dir}{dataset_name}_minsup_{support}.txt'\n",
    "    write_to_file(phase1_output, phase2_output, output_filepath)\n",
    "    \n",
    "    print(Fore.BLUE + f'--'*30)\n",
    "    print(Fore.WHITE + f\"| MINSUP:\",Fore.GREEN + f\"{support}\")\n",
    "    print(Fore.WHITE + f\"| Time taken:\", Fore.GREEN + f\"{round(time.time() - start, 1)} seconds.\")\n",
    "    print(Fore.WHITE + f\"| Outputs saved to\", Fore.GREEN +  f\"{output_filepath}.\")\n",
    "    print(Fore.BLUE + f'--'*30)\n",
    "    \n",
    "except Exception as e:\n",
    "    sc.stop()\n",
    "    print(Fore.RED + f'Spark Server stopped due to exception.')\n",
    "    print(Fore.RED + f'Error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mRunning SON Apriori Algorithm on \u001b[32mgroceries \u001b[37mwith minsup value: 150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m------------------------------------------------------------\n",
      "\u001b[37m| MINSUP: \u001b[32m150\n",
      "\u001b[37m| Time taken: \u001b[32m4.9 seconds.\n",
      "\u001b[37m| Outputs saved to \u001b[32m./logs/groceries/groceries_minsup_150.txt.\n",
      "\u001b[34m------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "phase1_candidates = []\n",
    "support = minsup_values_list[2]\n",
    "\n",
    "print(Fore.WHITE + f'Running SON Apriori Algorithm on', Fore.GREEN + f'{dataset_name}', \n",
    "        Fore.WHITE + f'with minsup value: {support}\\n')\n",
    "\n",
    "\n",
    "spark_configuration = SparkConf().setAppName(\"cz4032_task2\").setMaster('local[*]')\n",
    "sc = SparkContext(conf=spark_configuration)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "try:\n",
    "    rdd = sc.textFile(input_filepath)\n",
    "    \n",
    "\n",
    "    basket_rdd = rdd.filter(\n",
    "        lambda x: x != column_names).map(\n",
    "            lambda x: get_required_bucket(x)\n",
    "            ).groupByKey().mapValues(set).map(\n",
    "                lambda x: x[1]\n",
    "                ).persist()\n",
    "        \n",
    "    basket_list = basket_rdd.collect()\n",
    "    basket_count = len(basket_list)\n",
    "    \n",
    "    phase1_map = basket_rdd.mapPartitions(apriori).flatMap(lambda x: x)\n",
    "    phase1_reduce = phase1_map.reduceByKey(lambda x, y: x + y).persist()\n",
    "\n",
    "    phase2_map = phase1_reduce.map(lambda x: son(x[0])).flatMap(lambda x: x)\n",
    "    phase2_reduce = phase2_map.filter(lambda x: x[1] >= support).map(lambda x: x[0]).collect()\n",
    "\n",
    "    phase1_output = get_output(phase1_reduce, False)[:-1]\n",
    "    phase2_output = get_output(phase2_reduce, True)[:-1]\n",
    "\n",
    "    sc.stop()\n",
    "    \n",
    "    output_filepath = f'{output_dir}{dataset_name}_minsup_{support}.txt'\n",
    "    write_to_file(phase1_output, phase2_output, output_filepath)\n",
    "    \n",
    "    print(Fore.BLUE + f'--'*30)\n",
    "    print(Fore.WHITE + f\"| MINSUP:\",Fore.GREEN + f\"{support}\")\n",
    "    print(Fore.WHITE + f\"| Time taken:\", Fore.GREEN + f\"{round(time.time() - start, 1)} seconds.\")\n",
    "    print(Fore.WHITE + f\"| Outputs saved to\", Fore.GREEN +  f\"{output_filepath}.\")\n",
    "    print(Fore.BLUE + f'--'*30)\n",
    "    \n",
    "except Exception as e:\n",
    "    sc.stop()\n",
    "    print(Fore.RED + f'Spark Server stopped due to exception.')\n",
    "    print(Fore.RED + f'Error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mRunning SON Apriori Algorithm on \u001b[32mgroceries \u001b[37mwith minsup value: 100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m------------------------------------------------------------\n",
      "\u001b[37m| MINSUP: \u001b[32m100\n",
      "\u001b[37m| Time taken: \u001b[32m5.7 seconds.\n",
      "\u001b[37m| Outputs saved to \u001b[32m./logs/groceries/groceries_minsup_100.txt.\n",
      "\u001b[34m------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "phase1_candidates = []\n",
    "support = minsup_values_list[3]\n",
    "\n",
    "print(Fore.WHITE + f'Running SON Apriori Algorithm on', Fore.GREEN + f'{dataset_name}', \n",
    "        Fore.WHITE + f'with minsup value: {support}\\n')\n",
    "\n",
    "\n",
    "spark_configuration = SparkConf().setAppName(\"cz4032_task2\").setMaster('local[*]')\n",
    "sc = SparkContext(conf=spark_configuration)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "try:\n",
    "    rdd = sc.textFile(input_filepath)\n",
    "    \n",
    "\n",
    "    basket_rdd = rdd.filter(\n",
    "        lambda x: x != column_names).map(\n",
    "            lambda x: get_required_bucket(x)\n",
    "            ).groupByKey().mapValues(set).map(\n",
    "                lambda x: x[1]\n",
    "                ).persist()\n",
    "        \n",
    "    basket_list = basket_rdd.collect()\n",
    "    basket_count = len(basket_list)\n",
    "    \n",
    "    phase1_map = basket_rdd.mapPartitions(apriori).flatMap(lambda x: x)\n",
    "    phase1_reduce = phase1_map.reduceByKey(lambda x, y: x + y).persist()\n",
    "\n",
    "    phase2_map = phase1_reduce.map(lambda x: son(x[0])).flatMap(lambda x: x)\n",
    "    phase2_reduce = phase2_map.filter(lambda x: x[1] >= support).map(lambda x: x[0]).collect()\n",
    "\n",
    "    phase1_output = get_output(phase1_reduce, False)[:-1]\n",
    "    phase2_output = get_output(phase2_reduce, True)[:-1]\n",
    "\n",
    "    sc.stop()\n",
    "    \n",
    "    output_filepath = f'{output_dir}{dataset_name}_minsup_{support}.txt'\n",
    "    write_to_file(phase1_output, phase2_output, output_filepath)\n",
    "    \n",
    "    print(Fore.BLUE + f'--'*30)\n",
    "    print(Fore.WHITE + f\"| MINSUP:\",Fore.GREEN + f\"{support}\")\n",
    "    print(Fore.WHITE + f\"| Time taken:\", Fore.GREEN + f\"{round(time.time() - start, 1)} seconds.\")\n",
    "    print(Fore.WHITE + f\"| Outputs saved to\", Fore.GREEN +  f\"{output_filepath}.\")\n",
    "    print(Fore.BLUE + f'--'*30)\n",
    "    \n",
    "except Exception as e:\n",
    "    sc.stop()\n",
    "    print(Fore.RED + f'Spark Server stopped due to exception.')\n",
    "    print(Fore.RED + f'Error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mRunning SON Apriori Algorithm on \u001b[32mgroceries \u001b[37mwith minsup value: 50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m------------------------------------------------------------\n",
      "\u001b[37m| MINSUP: \u001b[32m50\n",
      "\u001b[37m| Time taken: \u001b[32m14.2 seconds.\n",
      "\u001b[37m| Outputs saved to \u001b[32m./logs/groceries/groceries_minsup_50.txt.\n",
      "\u001b[34m------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "phase1_candidates = []\n",
    "support = minsup_values_list[4]\n",
    "\n",
    "print(Fore.WHITE + f'Running SON Apriori Algorithm on', Fore.GREEN + f'{dataset_name}', \n",
    "        Fore.WHITE + f'with minsup value: {support}\\n')\n",
    "\n",
    "\n",
    "spark_configuration = SparkConf().setAppName(\"cz4032_task2\").setMaster('local[*]')\n",
    "sc = SparkContext(conf=spark_configuration)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "try:\n",
    "    rdd = sc.textFile(input_filepath)\n",
    "    \n",
    "\n",
    "    basket_rdd = rdd.filter(\n",
    "        lambda x: x != column_names).map(\n",
    "            lambda x: get_required_bucket(x)\n",
    "            ).groupByKey().mapValues(set).map(\n",
    "                lambda x: x[1]\n",
    "                ).persist()\n",
    "        \n",
    "    basket_list = basket_rdd.collect()\n",
    "    basket_count = len(basket_list)\n",
    "    \n",
    "    phase1_map = basket_rdd.mapPartitions(apriori).flatMap(lambda x: x)\n",
    "    phase1_reduce = phase1_map.reduceByKey(lambda x, y: x + y).persist()\n",
    "\n",
    "    phase2_map = phase1_reduce.map(lambda x: son(x[0])).flatMap(lambda x: x)\n",
    "    phase2_reduce = phase2_map.filter(lambda x: x[1] >= support).map(lambda x: x[0]).collect()\n",
    "\n",
    "    phase1_output = get_output(phase1_reduce, False)[:-1]\n",
    "    phase2_output = get_output(phase2_reduce, True)[:-1]\n",
    "\n",
    "    sc.stop()\n",
    "    \n",
    "    output_filepath = f'{output_dir}{dataset_name}_minsup_{support}.txt'\n",
    "    write_to_file(phase1_output, phase2_output, output_filepath)\n",
    "    \n",
    "    print(Fore.BLUE + f'--'*30)\n",
    "    print(Fore.WHITE + f\"| MINSUP:\",Fore.GREEN + f\"{support}\")\n",
    "    print(Fore.WHITE + f\"| Time taken:\", Fore.GREEN + f\"{round(time.time() - start, 1)} seconds.\")\n",
    "    print(Fore.WHITE + f\"| Outputs saved to\", Fore.GREEN +  f\"{output_filepath}.\")\n",
    "    print(Fore.BLUE + f'--'*30)\n",
    "    \n",
    "except Exception as e:\n",
    "    sc.stop()\n",
    "    print(Fore.RED + f'Spark Server stopped due to exception.')\n",
    "    print(Fore.RED + f'Error: {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dam_proj2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
