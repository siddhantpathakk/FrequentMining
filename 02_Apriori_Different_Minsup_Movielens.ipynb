{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Initialisation and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import itertools\n",
    "import time\n",
    "from colorama import Fore\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mining based utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_count_dict(items, support_value):\n",
    "    item_counts = {}\n",
    "    for item in items:\n",
    "        if item not in item_counts.keys():\n",
    "            item_counts[item] = 1\n",
    "        else:\n",
    "            if item_counts[item] < support_value:\n",
    "                item_counts[item] = item_counts[item] + 1\n",
    "    return item_counts\n",
    "\n",
    "\n",
    "def create_count_dict_tuple(chunk, support_value, candidate_tuples):\n",
    "    tuple_counts = {}\n",
    "    for basket in chunk:\n",
    "        for candidate in candidate_tuples:\n",
    "            if set(candidate).issubset(basket):\n",
    "                if candidate in tuple_counts and tuple_counts[candidate] < support_value:\n",
    "                    tuple_counts[candidate] += 1\n",
    "                elif candidate not in tuple_counts:\n",
    "                    tuple_counts[candidate] = 1\n",
    "    return tuple_counts\n",
    "\n",
    "\n",
    "def filter_by_support(count_dict, support_value, is_tuple):\n",
    "    frequent_candidates = []\n",
    "    for candidate, count in count_dict.items():\n",
    "        if count >= support_value:\n",
    "            frequent_candidates.append(candidate)\n",
    "            phase1_candidates.append((candidate if is_tuple else tuple({candidate}), 1))\n",
    "    return frequent_candidates\n",
    "\n",
    "\n",
    "def get_frequent_candidate_tuples(frequent_items, size):\n",
    "    candidates = []\n",
    "    for item_x in frequent_items:\n",
    "        for item_y in frequent_items:\n",
    "            combined_set = tuple(sorted(set(item_x + item_y)))\n",
    "            if len(combined_set) == size:\n",
    "                if combined_set not in candidates:\n",
    "                    previous_candidates = list(itertools.combinations(combined_set, size - 1))\n",
    "                    flag = True\n",
    "                    for candidate in previous_candidates:\n",
    "                        if candidate not in frequent_items:\n",
    "                            flag = False\n",
    "                            break\n",
    "                    if flag:\n",
    "                        candidates.append(combined_set)\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def find_frequent_candidates(chunk, chunk_support, frequent_items, size):\n",
    "    if size == 2:\n",
    "        candidates = list(itertools.combinations(sorted(frequent_items), 2))\n",
    "    else:\n",
    "        candidates = get_frequent_candidate_tuples(frequent_items, size)\n",
    "    return filter_by_support(create_count_dict_tuple(chunk, chunk_support, candidates), chunk_support, True)\n",
    "\n",
    "\n",
    "def apriori(baskets):\n",
    "    chunk = list(baskets)\n",
    "    chunk_support = support * (len(chunk) / basket_count)\n",
    "    items = []\n",
    "    for basket in chunk:\n",
    "        for item in basket:\n",
    "            items.append(item)\n",
    "\n",
    "    #frequent itemsets of size 1\n",
    "    frequent_items = filter_by_support(create_count_dict(items, chunk_support), chunk_support, False)\n",
    "    size = 2\n",
    "    while len(frequent_items) != 0:\n",
    "        frequent_items = find_frequent_candidates(chunk, chunk_support, frequent_items, size)\n",
    "        size += 1\n",
    "\n",
    "    return [phase1_candidates]\n",
    "\n",
    "def son(candidate):\n",
    "    frequent_itemset_count = {}\n",
    "    for basket in basket_list:\n",
    "        if isinstance(candidate, tuple):\n",
    "            if set(candidate).issubset(basket):\n",
    "                if candidate in frequent_itemset_count and frequent_itemset_count[candidate] < support:\n",
    "                    frequent_itemset_count[candidate] += 1\n",
    "                elif candidate not in frequent_itemset_count:\n",
    "                    frequent_itemset_count[candidate] = 1\n",
    "        else:\n",
    "            frequent_itemset_count = create_count_dict(basket, support)\n",
    "\n",
    "    frequent_itemsets_actual = []\n",
    "    for itemset, count in frequent_itemset_count.items():\n",
    "        frequent_itemsets_actual.append((itemset, count))\n",
    "    return frequent_itemsets_actual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I/O based utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_required_bucket(row):\n",
    "    line = row.split(\",\")\n",
    "    return line[0], line[1]\n",
    "\n",
    "\n",
    "def write_to_file(candidates, frequent_itemsets, output_file):\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write(\"Candidates:\\n\" + candidates + \"\\n\\nFrequent Itemsets:\\n\" + frequent_itemsets)\n",
    "\n",
    "\n",
    "def get_output_string(input_tuple, size, output):\n",
    "    if len(input_tuple) > size:\n",
    "        output = output[:-1] + \"\\n\\n\"\n",
    "\n",
    "    output = output + \"('\" + str(input_tuple[0]) + \"'),\" if len(input_tuple) == 1 else output + str(input_tuple) + \",\"\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_output(phase_rdd, is_phase_2):\n",
    "    size = 1\n",
    "    output = \"\"\n",
    "    sorted_list = sorted(sorted(phase_rdd if is_phase_2 else phase_rdd.map(lambda x: x[0]).collect()), key=len)\n",
    "    for x in sorted_list:\n",
    "        output = get_output_string(x, size, output)\n",
    "        size = len(x)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Datasets chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_metadata = {\n",
    "    'amazon-reviews': {\n",
    "        'path': './data/amazon-reviews/transactions.csv',\n",
    "        'output_dir': './logs/amazon-reviews/',\n",
    "        'minsup': [5000, 3000, 1500, 1000],\n",
    "    },\n",
    "    \n",
    "    \n",
    "    'groceries': {\n",
    "        'path': './data/groceries/transactions.csv',\n",
    "        'output_dir': './logs/groceries/',\n",
    "        'column_names': 'userId,itemName',\n",
    "        'minsup': [250, 200, 150, 100, 50],\n",
    "    },\n",
    "    \n",
    "    \n",
    "    'movielens': {\n",
    "        'path': './data/movielens/transactions.csv',\n",
    "        'output_dir': './logs/movielens/',\n",
    "        'column_names': 'userId,movieId',\n",
    "        'minsup': [300, 250, 200, 150, 100],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'movielens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300, 250, 200, 150, 100]\n"
     ]
    }
   ],
   "source": [
    "input_filepath = dataset_metadata[dataset_name]['path']\n",
    "output_dir = dataset_metadata[dataset_name]['output_dir']\n",
    "minsup_values_list = dataset_metadata[dataset_name]['minsup']\n",
    "column_names = dataset_metadata[dataset_name]['column_names']\n",
    "\n",
    "print(minsup_values_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mRunning SON Apriori Algorithm on \u001b[32mmovielens \u001b[37mwith minsup value: 300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/20 04:34:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m------------------------------------------------------------\n",
      "\u001b[37m| MINSUP: \u001b[32m300\n",
      "\u001b[37m| Time taken: \u001b[32m10.2 seconds.\n",
      "\u001b[37m| Outputs saved to \u001b[32m./logs/movielens/movielens_minsup_300.txt.\n",
      "\u001b[34m------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "phase1_candidates = []\n",
    "support = minsup_values_list[0]\n",
    "\n",
    "print(Fore.WHITE + f'Running SON Apriori Algorithm on', Fore.GREEN + f'{dataset_name}', \n",
    "        Fore.WHITE + f'with minsup value: {support}\\n')\n",
    "\n",
    "\n",
    "spark_configuration = SparkConf().setAppName(\"cz4032_task2\").setMaster('local[*]')\n",
    "sc = SparkContext(conf=spark_configuration)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "try:\n",
    "    rdd = sc.textFile(input_filepath)\n",
    "    \n",
    "\n",
    "    basket_rdd = rdd.filter(\n",
    "        lambda x: x != column_names).map(\n",
    "            lambda x: get_required_bucket(x)\n",
    "            ).groupByKey().mapValues(set).map(\n",
    "                lambda x: x[1]\n",
    "                ).persist()\n",
    "        \n",
    "    basket_list = basket_rdd.collect()\n",
    "    basket_count = len(basket_list)\n",
    "    \n",
    "    phase1_map = basket_rdd.mapPartitions(apriori).flatMap(lambda x: x)\n",
    "    phase1_reduce = phase1_map.reduceByKey(lambda x, y: x + y).persist()\n",
    "\n",
    "    phase2_map = phase1_reduce.map(lambda x: son(x[0])).flatMap(lambda x: x)\n",
    "    phase2_reduce = phase2_map.filter(lambda x: x[1] >= support).map(lambda x: x[0]).collect()\n",
    "\n",
    "    phase1_output = get_output(phase1_reduce, False)[:-1]\n",
    "    phase2_output = get_output(phase2_reduce, True)[:-1]\n",
    "\n",
    "    sc.stop()\n",
    "    \n",
    "    output_filepath = f'{output_dir}{dataset_name}_minsup_{support}.txt'\n",
    "    write_to_file(phase1_output, phase2_output, output_filepath)\n",
    "    \n",
    "    print(Fore.BLUE + f'--'*30)\n",
    "    print(Fore.WHITE + f\"| MINSUP:\",Fore.GREEN + f\"{support}\")\n",
    "    print(Fore.WHITE + f\"| Time taken:\", Fore.GREEN + f\"{round(time.time() - start, 1)} seconds.\")\n",
    "    print(Fore.WHITE + f\"| Outputs saved to\", Fore.GREEN +  f\"{output_filepath}.\")\n",
    "    print(Fore.BLUE + f'--'*30)\n",
    "    \n",
    "except Exception as e:\n",
    "    sc.stop()\n",
    "    print(Fore.RED + f'Spark Server stopped due to exception.')\n",
    "    print(Fore.RED + f'Error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mRunning SON Apriori Algorithm on \u001b[32mmovielens \u001b[37mwith minsup value: 250\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/20 04:35:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m------------------------------------------------------------\n",
      "\u001b[37m| MINSUP: \u001b[32m250\n",
      "\u001b[37m| Time taken: \u001b[32m10.0 seconds.\n",
      "\u001b[37m| Outputs saved to \u001b[32m./logs/movielens/movielens_minsup_250.txt.\n",
      "\u001b[34m------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "phase1_candidates = []\n",
    "support = minsup_values_list[1]\n",
    "\n",
    "print(Fore.WHITE + f'Running SON Apriori Algorithm on', Fore.GREEN + f'{dataset_name}', \n",
    "        Fore.WHITE + f'with minsup value: {support}\\n')\n",
    "\n",
    "\n",
    "spark_configuration = SparkConf().setAppName(\"cz4032_task2\").setMaster('local[*]')\n",
    "sc = SparkContext(conf=spark_configuration)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "try:\n",
    "    rdd = sc.textFile(input_filepath)\n",
    "    \n",
    "\n",
    "    basket_rdd = rdd.filter(\n",
    "        lambda x: x != column_names).map(\n",
    "            lambda x: get_required_bucket(x)\n",
    "            ).groupByKey().mapValues(set).map(\n",
    "                lambda x: x[1]\n",
    "                ).persist()\n",
    "        \n",
    "    basket_list = basket_rdd.collect()\n",
    "    basket_count = len(basket_list)\n",
    "    \n",
    "    phase1_map = basket_rdd.mapPartitions(apriori).flatMap(lambda x: x)\n",
    "    phase1_reduce = phase1_map.reduceByKey(lambda x, y: x + y).persist()\n",
    "\n",
    "    phase2_map = phase1_reduce.map(lambda x: son(x[0])).flatMap(lambda x: x)\n",
    "    phase2_reduce = phase2_map.filter(lambda x: x[1] >= support).map(lambda x: x[0]).collect()\n",
    "\n",
    "    phase1_output = get_output(phase1_reduce, False)[:-1]\n",
    "    phase2_output = get_output(phase2_reduce, True)[:-1]\n",
    "\n",
    "    sc.stop()\n",
    "    \n",
    "    output_filepath = f'{output_dir}{dataset_name}_minsup_{support}.txt'\n",
    "    write_to_file(phase1_output, phase2_output, output_filepath)\n",
    "    \n",
    "    print(Fore.BLUE + f'--'*30)\n",
    "    print(Fore.WHITE + f\"| MINSUP:\",Fore.GREEN + f\"{support}\")\n",
    "    print(Fore.WHITE + f\"| Time taken:\", Fore.GREEN + f\"{round(time.time() - start, 1)} seconds.\")\n",
    "    print(Fore.WHITE + f\"| Outputs saved to\", Fore.GREEN +  f\"{output_filepath}.\")\n",
    "    print(Fore.BLUE + f'--'*30)\n",
    "    \n",
    "except Exception as e:\n",
    "    sc.stop()\n",
    "    print(Fore.RED + f'Spark Server stopped due to exception.')\n",
    "    print(Fore.RED + f'Error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mRunning SON Apriori Algorithm on \u001b[32mmovielens \u001b[37mwith minsup value: 200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/20 04:36:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m------------------------------------------------------------\n",
      "\u001b[37m| MINSUP: \u001b[32m200\n",
      "\u001b[37m| Time taken: \u001b[32m10.8 seconds.\n",
      "\u001b[37m| Outputs saved to \u001b[32m./logs/movielens/movielens_minsup_200.txt.\n",
      "\u001b[34m------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "phase1_candidates = []\n",
    "support = minsup_values_list[2]\n",
    "\n",
    "print(Fore.WHITE + f'Running SON Apriori Algorithm on', Fore.GREEN + f'{dataset_name}', \n",
    "        Fore.WHITE + f'with minsup value: {support}\\n')\n",
    "\n",
    "\n",
    "spark_configuration = SparkConf().setAppName(\"cz4032_task2\").setMaster('local[*]')\n",
    "sc = SparkContext(conf=spark_configuration)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "try:\n",
    "    rdd = sc.textFile(input_filepath)\n",
    "    \n",
    "\n",
    "    basket_rdd = rdd.filter(\n",
    "        lambda x: x != column_names).map(\n",
    "            lambda x: get_required_bucket(x)\n",
    "            ).groupByKey().mapValues(set).map(\n",
    "                lambda x: x[1]\n",
    "                ).persist()\n",
    "        \n",
    "    basket_list = basket_rdd.collect()\n",
    "    basket_count = len(basket_list)\n",
    "    \n",
    "    phase1_map = basket_rdd.mapPartitions(apriori).flatMap(lambda x: x)\n",
    "    phase1_reduce = phase1_map.reduceByKey(lambda x, y: x + y).persist()\n",
    "\n",
    "    phase2_map = phase1_reduce.map(lambda x: son(x[0])).flatMap(lambda x: x)\n",
    "    phase2_reduce = phase2_map.filter(lambda x: x[1] >= support).map(lambda x: x[0]).collect()\n",
    "\n",
    "    phase1_output = get_output(phase1_reduce, False)[:-1]\n",
    "    phase2_output = get_output(phase2_reduce, True)[:-1]\n",
    "\n",
    "    sc.stop()\n",
    "    \n",
    "    output_filepath = f'{output_dir}{dataset_name}_minsup_{support}.txt'\n",
    "    write_to_file(phase1_output, phase2_output, output_filepath)\n",
    "    \n",
    "    print(Fore.BLUE + f'--'*30)\n",
    "    print(Fore.WHITE + f\"| MINSUP:\",Fore.GREEN + f\"{support}\")\n",
    "    print(Fore.WHITE + f\"| Time taken:\", Fore.GREEN + f\"{round(time.time() - start, 1)} seconds.\")\n",
    "    print(Fore.WHITE + f\"| Outputs saved to\", Fore.GREEN +  f\"{output_filepath}.\")\n",
    "    print(Fore.BLUE + f'--'*30)\n",
    "    \n",
    "except Exception as e:\n",
    "    sc.stop()\n",
    "    print(Fore.RED + f'Spark Server stopped due to exception.')\n",
    "    print(Fore.RED + f'Error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mRunning SON Apriori Algorithm on \u001b[32mmovielens \u001b[37mwith minsup value: 150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/20 04:37:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m------------------------------------------------------------\n",
      "\u001b[37m| MINSUP: \u001b[32m150\n",
      "\u001b[37m| Time taken: \u001b[32m14.2 seconds.\n",
      "\u001b[37m| Outputs saved to \u001b[32m./logs/movielens/movielens_minsup_150.txt.\n",
      "\u001b[34m------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "phase1_candidates = []\n",
    "support = minsup_values_list[3]\n",
    "\n",
    "print(Fore.WHITE + f'Running SON Apriori Algorithm on', Fore.GREEN + f'{dataset_name}', \n",
    "        Fore.WHITE + f'with minsup value: {support}\\n')\n",
    "\n",
    "\n",
    "spark_configuration = SparkConf().setAppName(\"cz4032_task2\").setMaster('local[*]')\n",
    "sc = SparkContext(conf=spark_configuration)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "try:\n",
    "    rdd = sc.textFile(input_filepath)\n",
    "    \n",
    "\n",
    "    basket_rdd = rdd.filter(\n",
    "        lambda x: x != column_names).map(\n",
    "            lambda x: get_required_bucket(x)\n",
    "            ).groupByKey().mapValues(set).map(\n",
    "                lambda x: x[1]\n",
    "                ).persist()\n",
    "        \n",
    "    basket_list = basket_rdd.collect()\n",
    "    basket_count = len(basket_list)\n",
    "    \n",
    "    phase1_map = basket_rdd.mapPartitions(apriori).flatMap(lambda x: x)\n",
    "    phase1_reduce = phase1_map.reduceByKey(lambda x, y: x + y).persist()\n",
    "\n",
    "    phase2_map = phase1_reduce.map(lambda x: son(x[0])).flatMap(lambda x: x)\n",
    "    phase2_reduce = phase2_map.filter(lambda x: x[1] >= support).map(lambda x: x[0]).collect()\n",
    "\n",
    "    phase1_output = get_output(phase1_reduce, False)[:-1]\n",
    "    phase2_output = get_output(phase2_reduce, True)[:-1]\n",
    "\n",
    "    sc.stop()\n",
    "    \n",
    "    output_filepath = f'{output_dir}{dataset_name}_minsup_{support}.txt'\n",
    "    write_to_file(phase1_output, phase2_output, output_filepath)\n",
    "    \n",
    "    print(Fore.BLUE + f'--'*30)\n",
    "    print(Fore.WHITE + f\"| MINSUP:\",Fore.GREEN + f\"{support}\")\n",
    "    print(Fore.WHITE + f\"| Time taken:\", Fore.GREEN + f\"{round(time.time() - start, 1)} seconds.\")\n",
    "    print(Fore.WHITE + f\"| Outputs saved to\", Fore.GREEN +  f\"{output_filepath}.\")\n",
    "    print(Fore.BLUE + f'--'*30)\n",
    "    \n",
    "except Exception as e:\n",
    "    sc.stop()\n",
    "    print(Fore.RED + f'Spark Server stopped due to exception.')\n",
    "    print(Fore.RED + f'Error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mRunning SON Apriori Algorithm on \u001b[32mmovielens \u001b[37mwith minsup value: 100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/20 04:38:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m------------------------------------------------------------\n",
      "\u001b[37m| MINSUP: \u001b[32m100\n",
      "\u001b[37m| Time taken: \u001b[32m13.5 seconds.\n",
      "\u001b[37m| Outputs saved to \u001b[32m./logs/movielens/movielens_minsup_100.txt.\n",
      "\u001b[34m------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "phase1_candidates = []\n",
    "support = minsup_values_list[4]\n",
    "\n",
    "print(Fore.WHITE + f'Running SON Apriori Algorithm on', Fore.GREEN + f'{dataset_name}', \n",
    "        Fore.WHITE + f'with minsup value: {support}\\n')\n",
    "\n",
    "\n",
    "spark_configuration = SparkConf().setAppName(\"cz4032_task2\").setMaster('local[*]')\n",
    "sc = SparkContext(conf=spark_configuration)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "try:\n",
    "    rdd = sc.textFile(input_filepath)\n",
    "    \n",
    "\n",
    "    basket_rdd = rdd.filter(\n",
    "        lambda x: x != column_names).map(\n",
    "            lambda x: get_required_bucket(x)\n",
    "            ).groupByKey().mapValues(set).map(\n",
    "                lambda x: x[1]\n",
    "                ).persist()\n",
    "        \n",
    "    basket_list = basket_rdd.collect()\n",
    "    basket_count = len(basket_list)\n",
    "    \n",
    "    phase1_map = basket_rdd.mapPartitions(apriori).flatMap(lambda x: x)\n",
    "    phase1_reduce = phase1_map.reduceByKey(lambda x, y: x + y).persist()\n",
    "\n",
    "    phase2_map = phase1_reduce.map(lambda x: son(x[0])).flatMap(lambda x: x)\n",
    "    phase2_reduce = phase2_map.filter(lambda x: x[1] >= support).map(lambda x: x[0]).collect()\n",
    "\n",
    "    phase1_output = get_output(phase1_reduce, False)[:-1]\n",
    "    phase2_output = get_output(phase2_reduce, True)[:-1]\n",
    "\n",
    "    sc.stop()\n",
    "    \n",
    "    output_filepath = f'{output_dir}{dataset_name}_minsup_{support}.txt'\n",
    "    write_to_file(phase1_output, phase2_output, output_filepath)\n",
    "    \n",
    "    print(Fore.BLUE + f'--'*30)\n",
    "    print(Fore.WHITE + f\"| MINSUP:\",Fore.GREEN + f\"{support}\")\n",
    "    print(Fore.WHITE + f\"| Time taken:\", Fore.GREEN + f\"{round(time.time() - start, 1)} seconds.\")\n",
    "    print(Fore.WHITE + f\"| Outputs saved to\", Fore.GREEN +  f\"{output_filepath}.\")\n",
    "    print(Fore.BLUE + f'--'*30)\n",
    "    \n",
    "except Exception as e:\n",
    "    sc.stop()\n",
    "    print(Fore.RED + f'Spark Server stopped due to exception.')\n",
    "    print(Fore.RED + f'Error: {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dam_proj2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
