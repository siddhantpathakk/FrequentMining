{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "import time\n",
    "\n",
    "from apriori_son_utils import apriori_partition, get_dataset_info, turnStr2Pair, countItemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_apriori_run(dataset_name, cur_minsup, partition_num, input_file_path):\n",
    "    start = time.time()\n",
    "    \n",
    "    sc = SparkContext(appName=f'cz4042_{dataset_name}_task2').getOrCreate()\n",
    "    \n",
    "    output_file_path = f'./logs/{dataset_name}/minsup_{\n",
    "        cur_minsup}_partition_{partition_num}.txt'\n",
    "    \n",
    "    try:\n",
    "        print('Start running Apriori algorithm on', dataset_name, 'with minsup =',\n",
    "              cur_minsup, 'and partition_num =', partition_num, '...')\n",
    "\n",
    "        with open(input_file_path) as f:\n",
    "            rawStrList = f.readlines()\n",
    "            pairList = [pair.split('\\n')[0] for pair in rawStrList]\n",
    "            f.close()\n",
    "\n",
    "        qualifiedUsersRDD = sc.parallelize(pairList, partition_num).map(\n",
    "            lambda pairStr: turnStr2Pair(pairStr)).groupByKey().mapValues(\n",
    "                lambda iterable: set(iterable)).map(\n",
    "                    lambda pair: pair[1])\n",
    "\n",
    "        full_size = qualifiedUsersRDD.count()\n",
    "\n",
    "        rawCandidates = qualifiedUsersRDD.mapPartitions(\n",
    "            lambda partition: apriori_partition(partition, cur_minsup, full_size))\n",
    "\n",
    "        candidatesResultRDD = rawCandidates.flatMap(lambda x: x).flatMap(\n",
    "            lambda x: x).distinct().sortBy(\n",
    "                lambda pairs: (len(pairs), pairs))\n",
    "\n",
    "        candidatesResult = candidatesResultRDD.collect()\n",
    "\n",
    "        busResult = qualifiedUsersRDD.collect()\n",
    "\n",
    "        frequentItemsets = candidatesResultRDD.map(\n",
    "            lambda cand: countItemsets(cand, busResult)).filter(\n",
    "                lambda itemset: itemset[1] >= cur_minsup).sortBy(\n",
    "                    lambda pair: (len(pair[0]), pair[0])).map(\n",
    "                        lambda pair: pair[0])\n",
    "\n",
    "        \n",
    "        with open(output_file_path, 'w') as f:\n",
    "            f.write('Candidates:\\n')\n",
    "            for cand in candidatesResult:\n",
    "                f.write(str(cand) + '\\n')\n",
    "            f.write('\\nFrequent Itemsets:\\n')\n",
    "            for itemset in frequentItemsets.collect():\n",
    "                f.write(str(itemset) + '\\n')\n",
    "            f.close()\n",
    "\n",
    "        sc.stop()\n",
    "\n",
    "        # print(frequentItemsets.collect())\n",
    "        print(f'Output file is saved to {output_file_path}')\n",
    "        print('Time taken:', round(time.time() - start, 2), 'seconds.')\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        sc.stop()\n",
    "        print('KeyboardInterrupt. Therefore, SparkContext server stopped.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(dataset_name):\n",
    "    info = get_dataset_info(dataset_name)\n",
    "    input_file_path = info['path']\n",
    "    cur_minsup = 0\n",
    "    minsup_step = info['minsup_step']\n",
    "    partition_num = info['partition_num']\n",
    "    \n",
    "    for i in range(5):\n",
    "        cur_minsup = cur_minsup + minsup_step\n",
    "        \n",
    "        # SON for partition based Apriori algorithm\n",
    "        spark_apriori_run(dataset_name, cur_minsup, partition_num, input_file_path)\n",
    "        \n",
    "        print('Finish running Apriori algorithm on', dataset_name, 'with minsup =',\n",
    "              cur_minsup, 'and partition_num =', partition_num)\n",
    "        \n",
    "        print('--'*50,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 13:00:51 WARN Utils: Your hostname, Siddhant-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.91.3.5 instead (on interface en0)\n",
      "23/11/20 13:00:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/20 13:00:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running Apriori algorithm on groceries with minsup = 50 and partition_num = 10 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file is saved to ./logs/groceries/minsup_50_partition_10.txt\n",
      "Time taken: 16.66 seconds.\n",
      "Finish running Apriori algorithm on groceries with minsup = 50 and partition_num = 10\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "Start running Apriori algorithm on groceries with minsup = 100 and partition_num = 10 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file is saved to ./logs/groceries/minsup_100_partition_10.txt\n",
      "Time taken: 5.06 seconds.\n",
      "Finish running Apriori algorithm on groceries with minsup = 100 and partition_num = 10\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "Start running Apriori algorithm on groceries with minsup = 150 and partition_num = 10 ...\n",
      "Output file is saved to ./logs/groceries/minsup_150_partition_10.txt\n",
      "Time taken: 3.64 seconds.\n",
      "Finish running Apriori algorithm on groceries with minsup = 150 and partition_num = 10\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "Start running Apriori algorithm on groceries with minsup = 200 and partition_num = 10 ...\n",
      "Output file is saved to ./logs/groceries/minsup_200_partition_10.txt\n",
      "Time taken: 3.29 seconds.\n",
      "Finish running Apriori algorithm on groceries with minsup = 200 and partition_num = 10\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "Start running Apriori algorithm on groceries with minsup = 250 and partition_num = 10 ...\n",
      "Output file is saved to ./logs/groceries/minsup_250_partition_10.txt\n",
      "Time taken: 3.15 seconds.\n",
      "Finish running Apriori algorithm on groceries with minsup = 250 and partition_num = 10\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "CPU times: user 372 ms, sys: 103 ms, total: 476 ms\n",
      "Wall time: 31.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "main(dataset_name='groceries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 13:39:03 WARN Utils: Your hostname, Siddhant-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.91.3.5 instead (on interface en0)\n",
      "23/11/20 13:39:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/20 13:39:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running Apriori algorithm on movielens with minsup = 100 and partition_num = 100 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.                       (0 + 8) / 100]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/siddhantpathak/anaconda3/envs/fp_spark/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=74>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/siddhantpathak/anaconda3/envs/fp_spark/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/siddhantpathak/anaconda3/envs/fp_spark/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/siddhantpathak/anaconda3/envs/fp_spark/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/siddhantpathak/anaconda3/envs/fp_spark/lib/python3.12/socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/siddhantpathak/anaconda3/envs/fp_spark/lib/python3.12/site-packages/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/Users/siddhantpathak/anaconda3/envs/fp_spark/lib/python3.12/site-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "    ^^^^^^^^^^^^^^\n",
      "  File \"/Users/siddhantpathak/anaconda3/envs/fp_spark/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/siddhantpathak/anaconda3/envs/fp_spark/lib/python3.12/site-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o13.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/siddhantpathak/anaconda3/envs/fp_spark/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/siddhantpathak/anaconda3/envs/fp_spark/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "\u001b[1;32m/Users/siddhantpathak/Desktop/Projects/FrequentMining/task2.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/siddhantpathak/Desktop/Projects/FrequentMining/task2.ipynb#X45sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m cur_minsup \u001b[39m=\u001b[39m cur_minsup \u001b[39m+\u001b[39m minsup_step\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/siddhantpathak/Desktop/Projects/FrequentMining/task2.ipynb#X45sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# SON for partition based Apriori algorithm\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/siddhantpathak/Desktop/Projects/FrequentMining/task2.ipynb#X45sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m spark_apriori_run(dataset_name, cur_minsup, partition_num, input_file_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/siddhantpathak/Desktop/Projects/FrequentMining/task2.ipynb#X45sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFinish running Apriori algorithm on\u001b[39m\u001b[39m'\u001b[39m, dataset_name, \u001b[39m'\u001b[39m\u001b[39mwith minsup =\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/siddhantpathak/Desktop/Projects/FrequentMining/task2.ipynb#X45sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m       cur_minsup, \u001b[39m'\u001b[39m\u001b[39mand partition_num =\u001b[39m\u001b[39m'\u001b[39m, partition_num)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/siddhantpathak/Desktop/Projects/FrequentMining/task2.ipynb#X45sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m--\u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m50\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/siddhantpathak/Desktop/Projects/FrequentMining/task2.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/siddhantpathak/Desktop/Projects/FrequentMining/task2.ipynb#X45sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m full_size \u001b[39m=\u001b[39m qualifiedUsersRDD\u001b[39m.\u001b[39mcount()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/siddhantpathak/Desktop/Projects/FrequentMining/task2.ipynb#X45sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m rawCandidates \u001b[39m=\u001b[39m qualifiedUsersRDD\u001b[39m.\u001b[39mmapPartitions(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/siddhantpathak/Desktop/Projects/FrequentMining/task2.ipynb#X45sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mlambda\u001b[39;00m partition: apriori_partition(partition, cur_minsup, full_size))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/siddhantpathak/Desktop/Projects/FrequentMining/task2.ipynb#X45sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m candidatesResultRDD \u001b[39m=\u001b[39m rawCandidates\u001b[39m.\u001b[39;49mflatMap(\u001b[39mlambda\u001b[39;49;00m x: x)\u001b[39m.\u001b[39;49mflatMap(\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/siddhantpathak/Desktop/Projects/FrequentMining/task2.ipynb#X45sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mlambda\u001b[39;49;00m x: x)\u001b[39m.\u001b[39;49mdistinct()\u001b[39m.\u001b[39;49msortBy(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/siddhantpathak/Desktop/Projects/FrequentMining/task2.ipynb#X45sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         \u001b[39mlambda\u001b[39;49;00m pairs: (\u001b[39mlen\u001b[39;49m(pairs), pairs))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/siddhantpathak/Desktop/Projects/FrequentMining/task2.ipynb#X45sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m candidatesResult \u001b[39m=\u001b[39m candidatesResultRDD\u001b[39m.\u001b[39mcollect()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/siddhantpathak/Desktop/Projects/FrequentMining/task2.ipynb#X45sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m busResult \u001b[39m=\u001b[39m qualifiedUsersRDD\u001b[39m.\u001b[39mcollect()\n",
      "File \u001b[0;32m~/anaconda3/envs/fp_spark/lib/python3.12/site-packages/pyspark/rdd.py:1573\u001b[0m, in \u001b[0;36mRDD.sortBy\u001b[0;34m(self, keyfunc, ascending, numPartitions)\u001b[0m\n\u001b[1;32m   1533\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msortBy\u001b[39m(\n\u001b[1;32m   1534\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mRDD[T]\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1535\u001b[0m     keyfunc: Callable[[T], \u001b[39m\"\u001b[39m\u001b[39mS\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   1536\u001b[0m     ascending: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   1537\u001b[0m     numPartitions: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1538\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRDD[T]\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1539\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1540\u001b[0m \u001b[39m    Sorts this RDD by the given keyfunc\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1569\u001b[0m \u001b[39m    [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\u001b[39;00m\n\u001b[1;32m   1570\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1571\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m   1572\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkeyBy(keyfunc)  \u001b[39m# type: ignore[type-var]\u001b[39;49;00m\n\u001b[0;32m-> 1573\u001b[0m         \u001b[39m.\u001b[39;49msortByKey(ascending, numPartitions)\n\u001b[1;32m   1574\u001b[0m         \u001b[39m.\u001b[39mvalues()\n\u001b[1;32m   1575\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/fp_spark/lib/python3.12/site-packages/pyspark/rdd.py:1509\u001b[0m, in \u001b[0;36mRDD.sortByKey\u001b[0;34m(self, ascending, numPartitions, keyfunc)\u001b[0m\n\u001b[1;32m   1504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmapPartitions(sortPartition, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1506\u001b[0m \u001b[39m# first compute the boundary of each part via sampling: we want to partition\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m# the key-space into bins such that the bins have roughly the same\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[39m# number of (key, value) pairs falling into them\u001b[39;00m\n\u001b[0;32m-> 1509\u001b[0m rddSize \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcount()\n\u001b[1;32m   1510\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m rddSize:\n\u001b[1;32m   1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m  \u001b[39m# empty RDD\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fp_spark/lib/python3.12/site-packages/pyspark/rdd.py:2316\u001b[0m, in \u001b[0;36mRDD.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2295\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcount\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m   2296\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2297\u001b[0m \u001b[39m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[1;32m   2298\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2314\u001b[0m \u001b[39m    3\u001b[39;00m\n\u001b[1;32m   2315\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2316\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(\u001b[39mlambda\u001b[39;49;00m i: [\u001b[39msum\u001b[39;49m(\u001b[39m1\u001b[39;49m \u001b[39mfor\u001b[39;49;00m _ \u001b[39min\u001b[39;49;00m i)])\u001b[39m.\u001b[39;49msum()\n",
      "File \u001b[0;32m~/anaconda3/envs/fp_spark/lib/python3.12/site-packages/pyspark/rdd.py:2291\u001b[0m, in \u001b[0;36mRDD.sum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mRDD[NumberOrArray]\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNumberOrArray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   2271\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2272\u001b[0m \u001b[39m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[1;32m   2273\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2289\u001b[0m \u001b[39m    6.0\u001b[39;00m\n\u001b[1;32m   2290\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(\u001b[39mlambda\u001b[39;49;00m x: [\u001b[39msum\u001b[39;49m(x)])\u001b[39m.\u001b[39;49mfold(  \u001b[39m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m   2292\u001b[0m         \u001b[39m0\u001b[39;49m, operator\u001b[39m.\u001b[39;49madd\n\u001b[1;32m   2293\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/fp_spark/lib/python3.12/site-packages/pyspark/rdd.py:2044\u001b[0m, in \u001b[0;36mRDD.fold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     \u001b[39myield\u001b[39;00m acc\n\u001b[1;32m   2041\u001b[0m \u001b[39m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[1;32m   2042\u001b[0m \u001b[39m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[1;32m   2043\u001b[0m \u001b[39m# to the final reduce call\u001b[39;00m\n\u001b[0;32m-> 2044\u001b[0m vals \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(func)\u001b[39m.\u001b[39;49mcollect()\n\u001b[1;32m   2045\u001b[0m \u001b[39mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n",
      "File \u001b[0;32m~/anaconda3/envs/fp_spark/lib/python3.12/site-packages/pyspark/rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext):\n\u001b[1;32m   1832\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mcollectAndServe(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39;49mrdd())\n\u001b[1;32m   1834\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/anaconda3/envs/fp_spark/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/fp_spark/lib/python3.12/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[39mtype\u001b[39m \u001b[39m=\u001b[39m answer[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                        (0 + 8) / 100]\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "main(dataset_name='movielens')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fp_spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
